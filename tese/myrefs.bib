%bibtex entries

@inproceedings{Broder:2015:BDN:2684822.2697027,
 author = {Broder, Andrei and Adamic, Lada and Franklin, Michael and Rijke, Maarten de and Xing, Eric and Yu, Kai},
 title = {Big Data: New Paradigm or "Sound and Fury, Signifying Nothing"?},
 booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
 series = {WSDM '15},
 year = {2015},
 isbn = {978-1-4503-3317-7},
 location = {Shanghai, China},
 pages = {5--6},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2684822.2697027},
 doi = {10.1145/2684822.2697027},
 acmid = {2697027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data},
}

@article{Prekopa2003,
abstract = {Probabilistic programming means two strongly connected models as well as the study of their mathematical properties, solutions of the relevant optimization problems and their applications. The two models are: maximizing (or minimizing) a probability under constraints and programming under probabilistic constraints. There are a number of variants and special cases of these models and we present them in Section 1. In Section 2 we summarize those mathematical theories which can be used to prove the convexity of large classes of our problems and we also show how they can be applied in this context. In Section 3 we present solution algorithms of our stochastic programming problems. Since we are handling probabilities of sets in higher dimensional spaces, it is necessary to use bounding and other approximation algorithms to find these probabilities with satisfactory precision. This is the subject of Section 5. In Section 4 we present two-stage and multi-stage problems which are combined with probabilistic constraints. Some duality and stability theorems are presented in Section 6. Finally, in Section 7, we present applications of our model constructions. © 2003 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1312.4328},
doi = {10.1016/S0927-0507(03)10005-9},
eprint = {1312.4328},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pr{\'{e}}kopa - 2003 - Probabilistic Programming.pdf:pdf},
isbn = {9781450328654},
issn = {09270507},
journal = {Handbooks in Operations Research and Management Science},
number = {C},
pages = {267--351},
title = {{Probabilistic Programming}},
volume = {10},
year = {2003}
}

@article{Kulkarni2015,
abstract = {Picture(a probabilistic programming language for scene understanding)을 통하여 complex한 vision model 도 연구할 수 있도록 face general-purpose inference machinery 를 통하여 자동으로 solving 할 수 있는 방법 소개 확률에 기반한 language scene을 임의의 2D/3D scene으로 표현할 수 있고 hierarchy representation layer를 통하여 simple pixel 이 아니라 좀더 abstract feature(contours ..)로 관찰된 object scene를 compare 할 수 있도록 하였다.},
author = {Kulkarni, Tejas D and Tenenbaum, Joshua B},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Tenenbaum - 2015 - Picture A Probabilistic Programming Language for Scene Perception.pdf:pdf},
journal = {Cvpr},
title = {{Picture : A Probabilistic Programming Language for Scene Perception}},
year = {2015}
}

@inproceedings{Amatriain:2013:BDU:2541176.2514691,
 author = {Amatriain, Xavier},
 title = {Beyond data: from user information to business value through personalized recommendations and consumer science},
 booktitle = {Proceedings of the 22nd ACM international conference on Conference on information \&\#38; knowledge management},
 series = {CIKM '13},
 year = {2013},
 isbn = {978-1-4503-2263-8},
 location = {San Francisco, California, USA},
 pages = {2199--2200},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2505515.2514691},
 doi = {10.1145/2505515.2514691},
 acmid = {2514691},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data, machine learning, personalization, recommender systems},
}

@article{Wagstaff2012,
abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field’s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
archivePrefix = {arXiv},
arxivId = {1206.4656},
author = {Wagstaff, Kiri},
doi = {10.1023/A:1007601113994},
eprint = {1206.4656},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagstaff - 2012 - Machine Learning that Matters.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {1067-5027},
journal = {Proceedings of the 29th International Conference on Machine Learning},
pages = {529--536},
pmid = {17823086},
title = {{Machine Learning that Matters}},
url = {http://icml.cc/discuss/2012/298.html},
year = {2012}
}

@Misc{SciPy,
 author = {Grisel, Olivier},
 title = {Keynote on 'Trends in Machine Learning and the SciPy community'},
 year = 2013,
 howpublished = {Available at https://youtu.be/S6IbD86Dbvc, accessed in 3rd Jan 2016},
}

@Misc{mlbrit,
 author = {Encyclopædia Britannica},
 title = {machine learning},
 year = 2016,
 howpublished = {Available at http://www.britannica.com/technology/machine-learning, accessed in 3rd Jan 2016},
}

@Misc{darpa,
 author = {Jagannathan, Suresh},
 title = {Probabilistic Programming for Advancing Machine Learning (PPAML)},
 year = 2013,
 howpublished = {Available at http://www.darpa.mil/program/probabilistic-programming-for-advancing-machine-Learning, accessed in 3rd Jan 2016},
}

@Misc{mlnot,
 author = {Schapire, Rob},
 title = {Lecture Notes in 'COS 511: Theoretical Machine Learning'},
 year = 2008,
 howpublished = {Available at http://www.cs.princeton.edu/courses/archive/spr08/cos511/scribe_notes/0204.pdf, accessed in 3rd Jan 2016},
}

@Misc{gartner,
 author = {Stamford, Conn},
 title = {Gartner's 2015 Hype Cycle for Emerging Technologies Identifies the Computing Innovations That Organizations Should Monitor},
 year = 2015,
 howpublished = {Available at http://www.gartner.com/newsroom/id/3114217, accessed in 3rd Jan 2016},
}

@Misc{msbn,
 author = {Microsoft Research},
 title = {Microsoft Bayesian Network Editor},
 year = 2010,
 howpublished = {Available at http://research.microsoft.com/en-us/um/redmond/groups/adapt/msbnx/, accessed in 10th Jan 2016},
}

@Misc{kdn,
 author = {Piatetsky, Gregory},
 title = {16th annual KDnuggets Software Poll},
 year = 2015,
 howpublished = {Available at http://www.kdnuggets.com/2015/05/poll-r-rapidminer-python-big-data-spark.html, accessed in 4th Jan 2016},
}

@Misc{figaro,
 author = {Reposa, Michael and Takata, Glenn},
 title = {Figaro},
 year = 2015,
 howpublished = {Available at https://github.com/p2t2/figaro, accessed in 14th Jan 2016},
}

@misc{dippl,
  title = {{The Design and Implementation of Probabilistic Programming Languages}},
  author = {Goodman, Noah D and Stuhlm\"{u}ller, Andreas},
  year = {2014},
  howpublished = {\url{http://dippl.org}},
  note = {Accessed: 2016-1-14}
}

@misc{msalg,
  title = {{http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Working%20with%20different%20inference%20algorithms.aspx}},
  author = {Microsoft Research},
  howpublished = {\url{http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Working%20with%20different%20inference%20algorithms.aspx}},
  note = {Accessed: 2016-01-14}
}

@article{DBLP:journals/corr/abs-1212-2991,
 author = {Shawn Hershey and
           Jeffrey Bernstein and
           Bill Bradley and
           Andrew Schweitzer and
           Noah Stein and
           Theophane Weber and
           Benjamin Vigoda},
 title = {Accelerating Inference: towards a full Language, Compiler and Hardware stack},
 journal = {CoRR},
 volume = {abs/1212.2991},
 year = {2012},
 ee = {http://arxiv.org/abs/1212.2991},
 bibsource = {DBLP, http://dblp.uni-trier.de} }

@article{Winn2005,
abstract = {This paper presents Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to a Bayesian Network. Like belief propagation, Variational Message Passing proceeds by passing messages between nodes in the graph and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing ad- ditional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Mes- sage Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be specified graphically and then solved variationally without recourse to coding.},
author = {Winn, John and Bishop, Cm and Jaakkola, T},
doi = {10.1007/s002130100880},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winn, Bishop, Jaakkola - 2005 - Variational Message Passing.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,variational methods},
pages = {661--694},
pmid = {11713616},
title = {{Variational Message Passing}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=15324435{\&}AN=18003399{\&}h=hVptawIRxY9bpe0GokNI9+sC/7JSK74XN+ztPcGsGNYZQY5RlIgU6Iy2WzbduRcHJXoqZMRPgfHZO18TF6s+Kw=={\&}crl=c},
volume = {6},
year = {2005}
}

@article{Jordan1996,
author = {Jordan, Michael I and Weiss, Yair},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan, Weiss - 1996 - Probabilistic inference in graphical models.pdf:pdf},
journal = {Lauritzen, S. L},
number = {510},
pages = {140--152},
title = {{Probabilistic inference in graphical models}},
volume = {16},
year = {1996}
}

@Book{intml,
 author    = "Alpaydin, Ethem",
 title     = "Introduction to Machine Learning",
 publisher = "MIT Press",
 edition   = "Second",
 year      = 2010,
 isbn      = "0-262-01243-X"
}

@Book{thbay,
 author    = "Downey, Allen B.",
 title     = "Think Bayes",
 publisher = "Green Tea Press",
 year      = 2012
}

@article{Duvenaud,
author = {Duvenaud, David and Lloyd, James},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duvenaud, Lloyd - Unknown - Introduction to probabilistic programming.pdf:pdf},
title = {{Introduction to probabilistic programming}}
}

@article{Dastani2002,
abstract = {This paper presents a perceptually motivated formal framework for effective visualization of relational data. In this framework, the intended structure of data and the perceptual structure of visualizations are formally and uniformly defined in terms of relations that are induced on data and visual elements by data and visual attributes, respectively. Visual attributes are analyzed and classified from the perceptual point of view and in terms of perceptual relations that they induce on visual elements. The presented framework satisfies a necessary condition for effective data visualizations. This condition is formulated in terms of a structure preserving map between the intended structure of data and the perceptual structure of visualization. © 2002 Published by Elsevier Science Ltd.},
author = {Dastani, M.},
doi = {10.1006/S1045-926X(02)00026-5},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dastani - 2002 - The role of visual perception in data visualization.pdf:pdf},
issn = {1045-926X},
journal = {Journal of Visual Languages and Computing},
mendeley-groups = {VPL},
number = {6},
pages = {601--622},
title = {{The role of visual perception in data visualization}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0036888248{\&}partnerID=40},
volume = {13},
year = {2002}
}

@article{Petre1999,
author = {Petre, Marian and Blackwell, Alan F},
doi = {10.1006/ijhc.1999.0267},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petre, Blackwell - 1999 - Mental imagery in program design and visual programming.pdf:pdf},
issn = {1071-5819},
journal = {Int. J. Hum.-Comput. Stud.},
mendeley-groups = {VPL},
number = {1},
pages = {7--30},
title = {{Mental imagery in program design and visual programming}},
url = {http://dl.acm.org/citation.cfm?id=325912.325946},
volume = {51},
year = {1999}
}

@article{empstud,
author = {Lewis, Clayton and Olson, Garay},
journal = {Empiral Studies of Programmers:Second Workshop},
pages = {248--263},
title = {{Can principles of cognition lower the barriers to programming?}},
volume = {17},
year = {1987}
}

@article{dfbeg,
author = {Anjaneyulu, K S R and Anderson, John R},
title = {{The Advantages of Data Flow Diagrams for Beginning Programming}},
year = {1992}
}

@article{Freer2012,
abstract = {We prove a computable version of the de Finetti theorem on exchangeable sequences of real random variables. As a consequence, exchangeable stochastic processes expressed in probabilistic functional programming languages can be automatically rewritten as procedures that do not modify non-local state. Along the way, we prove that a distribution on the unit interval is computable if and only if its moments are uniformly computable. ?? 2011 Elsevier B.V..},
archivePrefix = {arXiv},
arxivId = {0912.1072},
author = {Freer, Cameron E. and Roy, Daniel M.},
doi = {10.1016/j.apal.2011.06.011},
eprint = {0912.1072},
file = {:home/gcandal/Downloads/FreerRoyCDEFpreprint.pdf:pdf},
isbn = {3642030726},
issn = {01680072},
journal = {Annals of Pure and Applied Logic},
keywords = {Computable probability theory,Exchangeability,Mutation,Probabilistic programming languages,The de Finetti theorem},
mendeley-groups = {PPL},
number = {5},
pages = {530--546},
title = {{Computable de Finetti measures}},
url = {http://dx.doi.org/10.1016/j.apal.2011.06.011},
volume = {163},
year = {2012}
}

@unpublised{mapt,
title = {Machine learning for Big Data processing},
author = {Georgieva, Pétia},
year = {2016}
}

@unpublised{belfn,
title = {Ohio State CIS 730, lecture notes on 'Probabilistic Reasoning - Belief Networks'},
author = {Wang, DeLiang},
year = {2002}
}

@unpublised{intpp,
title = {Talk on 'Introduction to probabilistic programming'},
author = {Duvenaud, David and Lloyd, James},
year = {2013},
journal = {University of Cambridge}
}

@article{Lassiter2012,
author = {Lassiter, Daniel},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lassiter - 2012 - Probabilistic reasoning and statistical inference An introduction ( for linguists and philosophers ).pdf:pdf},
mendeley-groups = {ML/Prob/Bayes},
pages = {1--51},
title = {{Probabilistic reasoning and statistical inference : An introduction ( for linguists and philosophers )}},
year = {2012}
}

@article{Szolovits1993,
abstract = {Our 1978 paper [27] reviewed the artificial intelligence-based medical (AIM) diagnostic systems. Medical diagnosis is one of the earliest difficult intellectual domains to which AI applications were suggested, and one where success could (and still can) lead to benefit ... $\backslash$n},
author = {Szolovits, P and Pauker, S G},
doi = {http://dx.doi.org/10.1016/0004-3702(93)90183-C},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szolovits, Peter and Pauker - Unknown - Categorical and Probabilistic Reasoning in Medical Diagnosis.pdf:pdf},
isbn = {0004-3702},
issn = {0004-3702},
journal = {Artificial Intelligence},
mendeley-groups = {ML/Prob/Bayes},
title = {{Categorical and probabilistic reasoning in medicine revisited}},
url = {http://www.sciencedirect.com/science/article/pii/000437029390183C/pdf?md5=fd214645a831f1ee383be36a8a81b822{\&}pid=1-s2.0-000437029390183C-main.pdf{\&}{\_}valck=1},
year = {1993}
}

@article{Sedlmeier2001,
abstract = {The authors present and test a new method of teaching Bayesian reasoning, something about which previous teaching studies reported little success. Based on G. Gigerenzer and U. Hoffrage's (1995) ecological framework, the authors wrote a computerized tutorial program to train people to construct frequency representations (representation training) rather than to insert probabilities into Bayes's rule (rule training). Bayesian computations are simpler to perform with natural frequencies than with probabilities, and there are evolutionary reasons for assuming that cognitive algorithms have been developed to deal with natural frequencies. In 2 studies, the authors compared representation training with rule training; the criteria were an immediate learning effect, transfer to new problems, and long-term temporal stability. Rule training was as good in transfer as representation training, but representation training had a higher immediate learning effect and greater temporal stability.},
author = {Sedlmeier, P and Gigerenzer, G},
doi = {10.1037/0096-3445.130.3.380},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sedlmeier, Gigerenzer - 2001 - Teaching Bayesian reasoning in less than two hours.pdf:pdf},
isbn = {0096-3445 (Print)$\backslash$r0022-1015 (Linking)},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
mendeley-groups = {ML/Prob/Bayes},
number = {3},
pages = {380--400},
pmid = {11561916},
title = {{Teaching Bayesian reasoning in less than two hours.}},
volume = {130},
year = {2001}
}

@unpublised{reas,
author = {Geyer-Schulz, Andreas and Dyer, Chuck},
title = {Lecture notes on Stanford's 'CS63: Knowledge Representation and Reasoning - Chapter 10.1-10.2, 10.6'},
}

@article{Fienberg2006,
abstract = {While Bayes’ theorem has a 250-year history, and the method of inverse probability that flowed from it dominated statistical thinking into the twentieth century, the adjective “Bayesian” was not part of the statistical lexicon until relatively recently. This paper provides an overview of key Bayesian developments, beginning with Bayes’ posthumously published 1763 paper and continuing up through approximately 1970, including the period of time when “Bayesian” emerged as the label of choice for those who advocated Bayesian methods.},
author = {Fienberg, Stephen E.},
doi = {http://dx.doi.org/10.1214/06-BA101},
file = {:home/gcandal/Downloads/fienberg-BA-06-Bayesian.pdf:pdf},
issn = {1931-6690 (print), 1931-6690 (electronic)},
journal = {Bayesian Analysis},
keywords = {bayes,classical statistical methods,frequentist methods,inverse probability,neo-bayesian revival,s law of eponymy,stigler,subjective,theorem},
mendeley-groups = {ML/Prob/Bayes},
number = {1},
pages = {1--41},
title = {{When did Bayesian inference become “Bayesian{\&}{\#}034;?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.8632},
year = {2006}
}

@article{christensen1982experience,
  title={Experience and the base-rate fallacy},
  author={Christensen-Szalanski, Jay JJ and Beach, Lee Roy},
  journal={Organizational Behavior and Human Performance},
  volume={29},
  number={2},
  pages={270--278},
  year={1982},
  publisher={Elsevier}
}

@article{Højsgaard2013,
author = {H{\o}jsgaard, S{\o}ren},
file = {:home/gcandal/Downloads/gRain-intro.pdf:pdf},
journal = {Aalborg University},
mendeley-groups = {ML/Prob/Bayes},
number = {2012},
pages = {1--17},
title = {{Bayesian networks in R with the gRain package}},
url = {http://cran.r-project.org/web/packages/gRain/vignettes/gRain-intro.pdf},
volume = {46},
year = {2013}
}

@article{PPm,
abstract = {Probabilistic programs are usual functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of vari- ables in a program via observations. Models from diverse applica- tion areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as proba- bilistic programs. Probabilistic inference is the problem of computing an explicit representation of the probability distribution implicitly specified by a probabilistic program. Depending on the application, the desired output from inference may vary—we may want to estimate the expected value of some function f with respect to the distribution, or the mode of the distribution, or simply a set of samples drawn from the distribution. In this paper, we describe connections this research area called “Probabilistic Programming” has with programming languages and software engineering, and this includes language design, and the static and dynamic analysis of programs. We survey current state of the art and speculate on promising directions for future research.},
author = {{Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani}, Sriram K.},
file = {:home/gcandal/Downloads/fose-icse2014.pdf:pdf},
journal = {International Conference on Software Engineering (ICSE Future of Software Engineering)},
mendeley-groups = {PPL},
pages = {267--},
title = {{Probabilistic Programming}},
year = {2014}
}

@article{Poole2010,
abstract = {Pearl [2000, p. 26] attributes to Laplace [1814] the idea of a probabilistic model as a deterministic system with stochastic inputs. Pearl defines causal models in terms of deterministic systems with stochastic inputs. In this paper, I show how deterministic ...},
author = {Poole, David},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poole - 2010 - Probabilistic Programming Languages Independent Choices and Deterministic Systems.pdf:pdf},
journal = {Heuristics, Probability and Causality: A Tribute to Judea Pearl},
mendeley-groups = {PPL},
pages = {253--269},
title = {{Probabilistic Programming Languages: Independent Choices and Deterministic Systems}},
url = {http://www.cs.ubc.ca/{~}poole/papers/IndependentChoices.pdf},
year = {2010}
}

@article{Andrieu2003,
abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
annote = {Enough to get an intuition on MC methods and dig deeper into the subject if needed. Giving up on understanding Metropolis-Hastings in greater detail.},
archivePrefix = {arXiv},
arxivId = {1109.4435v1},
author = {Andrieu, Christophe and {De Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I.},
doi = {10.1023/A:1020281327116},
eprint = {1109.4435v1},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrieu et al. - 2003 - An introduction to MCMC for machine learning.pdf:pdf},
isbn = {08856125 (ISSN)},
issn = {08856125},
journal = {Machine Learning},
keywords = {MCMC,Markov chain Monte Carlo,Sampling,Stochastic algorithms},
mendeley-groups = {PPL},
number = {1-2},
pages = {5--43},
pmid = {178037200001},
title = {{An introduction to MCMC for machine learning}},
volume = {50},
year = {2003}
}

@article{minka2012infer,
  title={Infer .NET 2.5},
  author={Minka, Tom and Winn, John and Guiver, John and Knowles, David},
  journal={Microsoft Research Cambridge},
  year={2012}
}

@article{Freer2010,
abstract = {We investigate the class of computable probability distributions and explore the fundamental limitations of using this class to describe and compute conditional distributions. In addition to proving the existence of noncomputable conditional distributions, and thus ruling out the possibility of generic probabilistic inference algorithms (even inefficient ones), we highlight some positive results showing that posterior inference is possible in the presence of additional structure like exchangeability and noise, both of which are common in Bayesian hierarchical modeling. This theoretical work bears on the development of probabilistic programming languages (which enable the specification of complex probabilistic models) and their implementations (which can be used to perform Bayesian reasoning). The probabilistic programming approach is particularly well suited for defining infinite-dimensional, recursively-defined stochastic processes of the sort used in nonparametric Bayesian statistics. We present a new construction of the Mondrian process as a partition-valued Markov process in continuous time, which can be viewed as placing a distribution on an infinite kd-tree data structure.},
author = {Freer, Cameron E and Mansinghka, Vikash K and Roy, Daniel M},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freer, Mansinghka, Roy - 2010 - When are probabilistic programs probably computationally tractable.pdf:pdf},
journal = {NIPS 2010 Workshop on Monte Carlo Methods in Modern Applications},
mendeley-groups = {PPL},
pages = {1--9},
title = {{When are probabilistic programs probably computationally tractable?}},
year = {2010}
}

@article{Minka1999,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, Expectation Propagation, unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propaga- tion in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expec- tation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models showExpectation Propagation to be convincingly better than methods with similar computational cost: Laplaces method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P},
eprint = {1301.2294},
file = {:home/gcandal/Downloads/minka-ep-uai.pdf:pdf},
isbn = {1-55860-800-1},
journal = {Statistics},
mendeley-groups = {PPL},
number = {2},
pages = {362--369},
title = {{Expectation Propagation for Approximate Bayesian Inference F d}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1319{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {17},
year = {1999}
}

@article{Zhang1994,
abstract = {The general problem of computing posterior probabilities in Bayesian networds is NP-hard (Cooper 1990). However efficient algorithms are often possible for particular applications by exploiting problem structures. It is well understood that the key to the materialization of such a possibility is to make use of conditional independence and work with factorizations of joint probabilities rather than joint probabilities themselves. Differnent exact approaches can be characterized in terms of their choices of factorizations. We propose a new approach which adopts a straightforward way for factorizing joint probabilities. In comparison with the clique tree propagation approach, our approach is very simple. It allows the pruning of irrelevant variables, it accommodates changes to the knowledge base more easily. It is easier to implement. More importantly, it can be adapted to utilize both intercausal independence and conditional independence in one uniform framework. On the other hand, clique tree propagation is better in terms of facilitating pre-computations.},
author = {Zhang, Nevin L and Poole, D},
file = {:home/gcandal/Downloads/canai94.pdf:pdf},
issn = {0710-0825},
journal = {Proc. of the Tenth Canadian Conference on Artificial Intelligence},
keywords = {probabilistic graphical models},
mendeley-groups = {ML/Prob/Bayes},
pages = {171--178},
title = {{A simple approach to Bayesian network computations}},
url = {http://hdl.handle.net/1783.1/757},
year = {1994}
}

@article{Winn2005,
abstract = {This paper presents Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to a Bayesian Network. Like belief propagation, Variational Message Passing proceeds by passing messages between nodes in the graph and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing ad- ditional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Mes- sage Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be specified graphically and then solved variationally without recourse to coding.},
author = {Winn, John and Bishop, Cm and Jaakkola, T},
doi = {10.1007/s002130100880},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winn, Bishop, Jaakkola - 2005 - Variational Message Passing.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,variational methods},
mendeley-groups = {PPL},
pages = {661--694},
pmid = {11713616},
title = {{Variational Message Passing}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=15324435{\&}AN=18003399{\&}h=hVptawIRxY9bpe0GokNI9+sC/7JSK74XN+ztPcGsGNYZQY5RlIgU6Iy2WzbduRcHJXoqZMRPgfHZO18TF6s+Kw=={\&}crl=c},
volume = {6},
year = {2005}
}

@article{Minka1999,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, Expectation Propagation, unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propaga- tion in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expec- tation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models showExpectation Propagation to be convincingly better than methods with similar computational cost: Laplaces method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P},
eprint = {1301.2294},
file = {:home/gcandal/Downloads/minka-ep-uai.pdf:pdf},
isbn = {1-55860-800-1},
journal = {Statistics},
mendeley-groups = {PPL},
number = {2},
pages = {362--369},
title = {{Expectation Propagation for Approximate Bayesian Inference F d}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1319{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {17},
year = {1999}
}

@article{Shen2010,
abstract = {n recent work we have developed a novel variational inference method for partially observed systems governed by stochastic differential equations. In this paper we provide a comparison of the Variational Gaussian Process Smoother with an exact solution computed using a Hybrid Monte Carlo approach to path sampling, applied to a stochastic double well potential model. It is demonstrated that the variational smoother provides us a very accurate estimate of mean path while conditional variance is slightly underestimated. We conclude with some remarks as t o the advantages and disadvantages of the variational smoother.},
author = {Shen, Yuan and Archambeau, Cedric and Cornford, Dan and Opper, Manfred and Shawe-taylor, John and Barillec, Remi},
doi = {10.1007/s11265-008-0299-y},
file = {:home/gcandal/Downloads/jsps{\_}ys09{\_}web.pdf:pdf},
issn = {1939-8018},
journal = {Journal of Signal Processing Systems},
mendeley-groups = {PPL},
number = {1},
pages = {51--59},
title = {{A comparison of variational and Markov Chain Monte Carlo methods for inference in partially observed stochastic dynamic systems}},
volume = {61},
year = {2010}
}
