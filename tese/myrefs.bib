%bibtex entries

@inproceedings{Broder:2015:BDN:2684822.2697027,
 author = {Broder, Andrei and Adamic, Lada and Franklin, Michael and Rijke, Maarten de and Xing, Eric and Yu, Kai},
 title = {Big Data: New Paradigm or "Sound and Fury, Signifying Nothing"?},
 booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
 series = {WSDM '15},
 year = {2015},
 isbn = {978-1-4503-3317-7},
 location = {Shanghai, China},
 pages = {5--6},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2684822.2697027},
 doi = {10.1145/2684822.2697027},
 acmid = {2697027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data},
}

@article{Prekopa2003,
abstract = {Probabilistic programming means two strongly connected models as well as the study of their mathematical properties, solutions of the relevant optimization problems and their applications. The two models are: maximizing (or minimizing) a probability under constraints and programming under probabilistic constraints. There are a number of variants and special cases of these models and we present them in Section 1. In Section 2 we summarize those mathematical theories which can be used to prove the convexity of large classes of our problems and we also show how they can be applied in this context. In Section 3 we present solution algorithms of our stochastic programming problems. Since we are handling probabilities of sets in higher dimensional spaces, it is necessary to use bounding and other approximation algorithms to find these probabilities with satisfactory precision. This is the subject of Section 5. In Section 4 we present two-stage and multi-stage problems which are combined with probabilistic constraints. Some duality and stability theorems are presented in Section 6. Finally, in Section 7, we present applications of our model constructions. © 2003 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1312.4328},
doi = {10.1016/S0927-0507(03)10005-9},
eprint = {1312.4328},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pr{\'{e}}kopa - 2003 - Probabilistic Programming.pdf:pdf},
isbn = {9781450328654},
issn = {09270507},
journal = {Handbooks in Operations Research and Management Science},
number = {C},
pages = {267--351},
title = {{Probabilistic Programming}},
volume = {10},
year = {2003}
}

@article{Kulkarni2015,
abstract = {Picture(a probabilistic programming language for scene understanding)을 통하여 complex한 vision model 도 연구할 수 있도록 face general-purpose inference machinery 를 통하여 자동으로 solving 할 수 있는 방법 소개 확률에 기반한 language scene을 임의의 2D/3D scene으로 표현할 수 있고 hierarchy representation layer를 통하여 simple pixel 이 아니라 좀더 abstract feature(contours ..)로 관찰된 object scene를 compare 할 수 있도록 하였다.},
author = {Kulkarni, Tejas D and Tenenbaum, Joshua B},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Tenenbaum - 2015 - Picture A Probabilistic Programming Language for Scene Perception.pdf:pdf},
journal = {Cvpr},
title = {{Picture : A Probabilistic Programming Language for Scene Perception}},
year = {2015}
}

@inproceedings{Amatriain:2013:BDU:2541176.2514691,
 author = {Amatriain, Xavier},
 title = {Beyond data: from user information to business value through personalized recommendations and consumer science},
 booktitle = {Proceedings of the 22nd ACM international conference on Conference on information \&\#38; knowledge management},
 series = {CIKM '13},
 year = {2013},
 isbn = {978-1-4503-2263-8},
 location = {San Francisco, California, USA},
 pages = {2199--2200},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2505515.2514691},
 doi = {10.1145/2505515.2514691},
 acmid = {2514691},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data, machine learning, personalization, recommender systems},
}

@article{Wagstaff2012,
abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field’s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
archivePrefix = {arXiv},
arxivId = {1206.4656},
author = {Wagstaff, Kiri},
doi = {10.1023/A:1007601113994},
eprint = {1206.4656},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagstaff - 2012 - Machine Learning that Matters.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {1067-5027},
journal = {Proceedings of the 29th International Conference on Machine Learning},
pages = {529--536},
pmid = {17823086},
title = {{Machine Learning that Matters}},
url = {http://icml.cc/discuss/2012/298.html},
year = {2012}
}

@Misc{SciPy,
 author = {Grisel, Olivier},
 title = {Keynote on 'Trends in Machine Learning and the SciPy community'},
 year = 2013,
 howpublished = {Available at https://youtu.be/S6IbD86Dbvc, accessed in 3rd Jan 2016},
}

@Misc{mlbrit,
 author = {Encyclopædia Britannica},
 title = {machine learning},
 year = 2016,
 howpublished = {Available at http://www.britannica.com/technology/machine-learning, accessed in 3rd Jan 2016},
}

@Misc{darpa,
 author = {Jagannathan, Suresh},
 title = {Probabilistic Programming for Advancing Machine Learning (PPAML)},
 year = 2013,
 howpublished = {Available at http://www.darpa.mil/program/probabilistic-programming-for-advancing-machine-Learning, accessed in 3rd Jan 2016},
}

@Misc{mlnot,
 author = {Schapire, Rob},
 title = {Lecture Notes in 'COS 511: Theoretical Machine Learning'},
 year = 2008,
 howpublished = {Available at http://www.cs.princeton.edu/courses/archive/spr08/cos511/scribe_notes/0204.pdf, accessed in 3rd Jan 2016},
}

@Misc{gartner,
 author = {Stamford, Conn},
 title = {Gartner's 2015 Hype Cycle for Emerging Technologies Identifies the Computing Innovations That Organizations Should Monitor},
 year = 2015,
 howpublished = {Available at http://www.gartner.com/newsroom/id/3114217, accessed in 3rd Jan 2016},
}

@Misc{msbn,
 author = {Microsoft Research},
 title = {Microsoft Bayesian Network Editor},
 year = 2010,
 howpublished = {Available at http://research.microsoft.com/en-us/um/redmond/groups/adapt/msbnx/, accessed in 10th Jan 2016},
}

@Misc{kdn,
 author = {Piatetsky, Gregory},
 title = {16th annual KDnuggets Software Poll},
 year = 2015,
 howpublished = {Available at http://www.kdnuggets.com/2015/05/poll-r-rapidminer-python-big-data-spark.html, accessed in 4th Jan 2016},
}

@Misc{figaro,
 author = {Reposa, Michael and Takata, Glenn},
 title = {Figaro},
 year = 2015,
 howpublished = {Available at https://github.com/p2t2/figaro, accessed in 14th Jan 2016},
}

@misc{dippl,
  title = {{The Design and Implementation of Probabilistic Programming Languages}},
  author = {Goodman, Noah D and Stuhlm\"{u}ller, Andreas},
  year = {2014},
  howpublished = {\url{http://dippl.org}},
  note = {Accessed: 2016-1-14}
}

@misc{msalg,
  title = {{http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Working%20with%20different%20inference%20algorithms.aspx}},
  author = {Microsoft Research},
  howpublished = {\url{http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Working%20with%20different%20inference%20algorithms.aspx}},
  note = {Accessed: 2016-01-14}
}

@article{DBLP:journals/corr/abs-1212-2991,
 author = {Shawn Hershey and
           Jeffrey Bernstein and
           Bill Bradley and
           Andrew Schweitzer and
           Noah Stein and
           Theophane Weber and
           Benjamin Vigoda},
 title = {Accelerating Inference: towards a full Language, Compiler and Hardware stack},
 journal = {CoRR},
 volume = {abs/1212.2991},
 year = {2012},
 ee = {http://arxiv.org/abs/1212.2991},
 bibsource = {DBLP, http://dblp.uni-trier.de} }

@article{Winn2005,
abstract = {This paper presents Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to a Bayesian Network. Like belief propagation, Variational Message Passing proceeds by passing messages between nodes in the graph and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing ad- ditional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Mes- sage Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be specified graphically and then solved variationally without recourse to coding.},
author = {Winn, John and Bishop, Cm and Jaakkola, T},
doi = {10.1007/s002130100880},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winn, Bishop, Jaakkola - 2005 - Variational Message Passing.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,variational methods},
pages = {661--694},
pmid = {11713616},
title = {{Variational Message Passing}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=15324435{\&}AN=18003399{\&}h=hVptawIRxY9bpe0GokNI9+sC/7JSK74XN+ztPcGsGNYZQY5RlIgU6Iy2WzbduRcHJXoqZMRPgfHZO18TF6s+Kw=={\&}crl=c},
volume = {6},
year = {2005}
}

@article{Jordan1996,
author = {Jordan, Michael I and Weiss, Yair},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan, Weiss - 1996 - Probabilistic inference in graphical models.pdf:pdf},
journal = {Lauritzen, S. L},
number = {510},
pages = {140--152},
title = {{Probabilistic inference in graphical models}},
volume = {16},
year = {1996}
}

@Book{intml,
 author    = "Alpaydin, Ethem",
 title     = "Introduction to Machine Learning",
 publisher = "MIT Press",
 edition   = "Second",
 year      = 2010,
 isbn      = "0-262-01243-X"
}

@Book{thbay,
 author    = "Downey, Allen B.",
 title     = "Think Bayes",
 publisher = "Green Tea Press",
 year      = 2012
}

@article{Duvenaud,
author = {Duvenaud, David and Lloyd, James},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duvenaud, Lloyd - Unknown - Introduction to probabilistic programming.pdf:pdf},
title = {{Introduction to probabilistic programming}}
}

@article{Dastani2002,
abstract = {This paper presents a perceptually motivated formal framework for effective visualization of relational data. In this framework, the intended structure of data and the perceptual structure of visualizations are formally and uniformly defined in terms of relations that are induced on data and visual elements by data and visual attributes, respectively. Visual attributes are analyzed and classified from the perceptual point of view and in terms of perceptual relations that they induce on visual elements. The presented framework satisfies a necessary condition for effective data visualizations. This condition is formulated in terms of a structure preserving map between the intended structure of data and the perceptual structure of visualization. © 2002 Published by Elsevier Science Ltd.},
author = {Dastani, M.},
doi = {10.1006/S1045-926X(02)00026-5},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dastani - 2002 - The role of visual perception in data visualization.pdf:pdf},
issn = {1045-926X},
journal = {Journal of Visual Languages and Computing},
mendeley-groups = {VPL},
number = {6},
pages = {601--622},
title = {{The role of visual perception in data visualization}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0036888248{\&}partnerID=40},
volume = {13},
year = {2002}
}

@article{Petre1999,
author = {Petre, Marian and Blackwell, Alan F},
doi = {10.1006/ijhc.1999.0267},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petre, Blackwell - 1999 - Mental imagery in program design and visual programming.pdf:pdf},
issn = {1071-5819},
journal = {Int. J. Hum.-Comput. Stud.},
mendeley-groups = {VPL},
number = {1},
pages = {7--30},
title = {{Mental imagery in program design and visual programming}},
url = {http://dl.acm.org/citation.cfm?id=325912.325946},
volume = {51},
year = {1999}
}

@article{empstud,
author = {Lewis, Clayton and Olson, Garay},
journal = {Empiral Studies of Programmers:Second Workshop},
pages = {248--263},
title = {{Can principles of cognition lower the barriers to programming?}},
volume = {17},
year = {1987}
}

@article{dfbeg,
author = {Anjaneyulu, K S R and Anderson, John R},
title = {{The Advantages of Data Flow Diagrams for Beginning Programming}},
year = {1992}
}

@article{Freer2012,
abstract = {We prove a computable version of the de Finetti theorem on exchangeable sequences of real random variables. As a consequence, exchangeable stochastic processes expressed in probabilistic functional programming languages can be automatically rewritten as procedures that do not modify non-local state. Along the way, we prove that a distribution on the unit interval is computable if and only if its moments are uniformly computable. ?? 2011 Elsevier B.V..},
archivePrefix = {arXiv},
arxivId = {0912.1072},
author = {Freer, Cameron E. and Roy, Daniel M.},
doi = {10.1016/j.apal.2011.06.011},
eprint = {0912.1072},
file = {:home/gcandal/Downloads/FreerRoyCDEFpreprint.pdf:pdf},
isbn = {3642030726},
issn = {01680072},
journal = {Annals of Pure and Applied Logic},
keywords = {Computable probability theory,Exchangeability,Mutation,Probabilistic programming languages,The de Finetti theorem},
mendeley-groups = {PPL},
number = {5},
pages = {530--546},
title = {{Computable de Finetti measures}},
url = {http://dx.doi.org/10.1016/j.apal.2011.06.011},
volume = {163},
year = {2012}
}

@unpublised{mapt,
title = {Machine learning for Big Data processing},
author = {Georgieva, Pétia},
year = {2016}
}

@unpublised{belfn,
title = {Ohio State CIS 730, lecture notes on 'Probabilistic Reasoning - Belief Networks'},
author = {Wang, DeLiang},
year = {2002}
}

@unpublised{intpp,
title = {Talk on 'Introduction to probabilistic programming'},
author = {Duvenaud, David and Lloyd, James},
year = {2013},
journal = {University of Cambridge}
}

@article{Lassiter2012,
author = {Lassiter, Daniel},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lassiter - 2012 - Probabilistic reasoning and statistical inference An introduction ( for linguists and philosophers ).pdf:pdf},
mendeley-groups = {ML/Prob/Bayes},
pages = {1--51},
title = {{Probabilistic reasoning and statistical inference : An introduction ( for linguists and philosophers )}},
year = {2012}
}

@article{Szolovits1993,
abstract = {Our 1978 paper [27] reviewed the artificial intelligence-based medical (AIM) diagnostic systems. Medical diagnosis is one of the earliest difficult intellectual domains to which AI applications were suggested, and one where success could (and still can) lead to benefit ... $\backslash$n},
author = {Szolovits, P and Pauker, S G},
doi = {http://dx.doi.org/10.1016/0004-3702(93)90183-C},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szolovits, Peter and Pauker - Unknown - Categorical and Probabilistic Reasoning in Medical Diagnosis.pdf:pdf},
isbn = {0004-3702},
issn = {0004-3702},
journal = {Artificial Intelligence},
mendeley-groups = {ML/Prob/Bayes},
title = {{Categorical and probabilistic reasoning in medicine revisited}},
url = {http://www.sciencedirect.com/science/article/pii/000437029390183C/pdf?md5=fd214645a831f1ee383be36a8a81b822{\&}pid=1-s2.0-000437029390183C-main.pdf{\&}{\_}valck=1},
year = {1993}
}

@article{Sedlmeier2001,
abstract = {The authors present and test a new method of teaching Bayesian reasoning, something about which previous teaching studies reported little success. Based on G. Gigerenzer and U. Hoffrage's (1995) ecological framework, the authors wrote a computerized tutorial program to train people to construct frequency representations (representation training) rather than to insert probabilities into Bayes's rule (rule training). Bayesian computations are simpler to perform with natural frequencies than with probabilities, and there are evolutionary reasons for assuming that cognitive algorithms have been developed to deal with natural frequencies. In 2 studies, the authors compared representation training with rule training; the criteria were an immediate learning effect, transfer to new problems, and long-term temporal stability. Rule training was as good in transfer as representation training, but representation training had a higher immediate learning effect and greater temporal stability.},
author = {Sedlmeier, P and Gigerenzer, G},
doi = {10.1037/0096-3445.130.3.380},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sedlmeier, Gigerenzer - 2001 - Teaching Bayesian reasoning in less than two hours.pdf:pdf},
isbn = {0096-3445 (Print)$\backslash$r0022-1015 (Linking)},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
mendeley-groups = {ML/Prob/Bayes},
number = {3},
pages = {380--400},
pmid = {11561916},
title = {{Teaching Bayesian reasoning in less than two hours.}},
volume = {130},
year = {2001}
}

@unpublised{reas,
author = {Geyer-Schulz, Andreas and Dyer, Chuck},
title = {Lecture notes on Stanford's 'CS63: Knowledge Representation and Reasoning - Chapter 10.1-10.2, 10.6'},
}

@article{Fienberg2006,
abstract = {While Bayes’ theorem has a 250-year history, and the method of inverse probability that flowed from it dominated statistical thinking into the twentieth century, the adjective “Bayesian” was not part of the statistical lexicon until relatively recently. This paper provides an overview of key Bayesian developments, beginning with Bayes’ posthumously published 1763 paper and continuing up through approximately 1970, including the period of time when “Bayesian” emerged as the label of choice for those who advocated Bayesian methods.},
author = {Fienberg, Stephen E.},
doi = {http://dx.doi.org/10.1214/06-BA101},
file = {:home/gcandal/Downloads/fienberg-BA-06-Bayesian.pdf:pdf},
issn = {1931-6690 (print), 1931-6690 (electronic)},
journal = {Bayesian Analysis},
keywords = {bayes,classical statistical methods,frequentist methods,inverse probability,neo-bayesian revival,s law of eponymy,stigler,subjective,theorem},
mendeley-groups = {ML/Prob/Bayes},
number = {1},
pages = {1--41},
title = {{When did Bayesian inference become “Bayesian{\&}{\#}034;?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.8632},
year = {2006}
}

@article{christensen1982experience,
  title={Experience and the base-rate fallacy},
  author={Christensen-Szalanski, Jay JJ and Beach, Lee Roy},
  journal={Organizational Behavior and Human Performance},
  volume={29},
  number={2},
  pages={270--278},
  year={1982},
  publisher={Elsevier}
}

@article{Højsgaard2013,
author = {H{\o}jsgaard, S{\o}ren},
file = {:home/gcandal/Downloads/gRain-intro.pdf:pdf},
journal = {Aalborg University},
mendeley-groups = {ML/Prob/Bayes},
number = {2012},
pages = {1--17},
title = {{Bayesian networks in R with the gRain package}},
url = {http://cran.r-project.org/web/packages/gRain/vignettes/gRain-intro.pdf},
volume = {46},
year = {2013}
}

@article{PPm,
abstract = {Probabilistic programs are usual functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of vari- ables in a program via observations. Models from diverse applica- tion areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as proba- bilistic programs. Probabilistic inference is the problem of computing an explicit representation of the probability distribution implicitly specified by a probabilistic program. Depending on the application, the desired output from inference may vary—we may want to estimate the expected value of some function f with respect to the distribution, or the mode of the distribution, or simply a set of samples drawn from the distribution. In this paper, we describe connections this research area called “Probabilistic Programming” has with programming languages and software engineering, and this includes language design, and the static and dynamic analysis of programs. We survey current state of the art and speculate on promising directions for future research.},
author = {{Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani}, Sriram K.},
file = {:home/gcandal/Downloads/fose-icse2014.pdf:pdf},
journal = {International Conference on Software Engineering (ICSE Future of Software Engineering)},
mendeley-groups = {PPL},
pages = {267--},
title = {{Probabilistic Programming}},
year = {2014}
}

@article{Poole2010,
abstract = {Pearl [2000, p. 26] attributes to Laplace [1814] the idea of a probabilistic model as a deterministic system with stochastic inputs. Pearl defines causal models in terms of deterministic systems with stochastic inputs. In this paper, I show how deterministic ...},
author = {Poole, David},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poole - 2010 - Probabilistic Programming Languages Independent Choices and Deterministic Systems.pdf:pdf},
journal = {Heuristics, Probability and Causality: A Tribute to Judea Pearl},
mendeley-groups = {PPL},
pages = {253--269},
title = {{Probabilistic Programming Languages: Independent Choices and Deterministic Systems}},
url = {http://www.cs.ubc.ca/{~}poole/papers/IndependentChoices.pdf},
year = {2010}
}

@article{Andrieu2003,
abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
annote = {Enough to get an intuition on MC methods and dig deeper into the subject if needed. Giving up on understanding Metropolis-Hastings in greater detail.},
archivePrefix = {arXiv},
arxivId = {1109.4435v1},
author = {Andrieu, Christophe and {De Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I.},
doi = {10.1023/A:1020281327116},
eprint = {1109.4435v1},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrieu et al. - 2003 - An introduction to MCMC for machine learning.pdf:pdf},
isbn = {08856125 (ISSN)},
issn = {08856125},
journal = {Machine Learning},
keywords = {MCMC,Markov chain Monte Carlo,Sampling,Stochastic algorithms},
mendeley-groups = {PPL},
number = {1-2},
pages = {5--43},
pmid = {178037200001},
title = {{An introduction to MCMC for machine learning}},
volume = {50},
year = {2003}
}

@article{minka2012infer,
  title={Infer .NET 2.5},
  author={Minka, Tom and Winn, John and Guiver, John and Knowles, David},
  journal={Microsoft Research Cambridge},
  year={2012}
}

@article{Freer2010,
abstract = {We investigate the class of computable probability distributions and explore the fundamental limitations of using this class to describe and compute conditional distributions. In addition to proving the existence of noncomputable conditional distributions, and thus ruling out the possibility of generic probabilistic inference algorithms (even inefficient ones), we highlight some positive results showing that posterior inference is possible in the presence of additional structure like exchangeability and noise, both of which are common in Bayesian hierarchical modeling. This theoretical work bears on the development of probabilistic programming languages (which enable the specification of complex probabilistic models) and their implementations (which can be used to perform Bayesian reasoning). The probabilistic programming approach is particularly well suited for defining infinite-dimensional, recursively-defined stochastic processes of the sort used in nonparametric Bayesian statistics. We present a new construction of the Mondrian process as a partition-valued Markov process in continuous time, which can be viewed as placing a distribution on an infinite kd-tree data structure.},
author = {Freer, Cameron E and Mansinghka, Vikash K and Roy, Daniel M},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freer, Mansinghka, Roy - 2010 - When are probabilistic programs probably computationally tractable.pdf:pdf},
journal = {NIPS 2010 Workshop on Monte Carlo Methods in Modern Applications},
mendeley-groups = {PPL},
pages = {1--9},
title = {{When are probabilistic programs probably computationally tractable?}},
year = {2010}
}

@article{Minka1999,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, Expectation Propagation, unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propaga- tion in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expec- tation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models showExpectation Propagation to be convincingly better than methods with similar computational cost: Laplaces method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P},
eprint = {1301.2294},
file = {:home/gcandal/Downloads/minka-ep-uai.pdf:pdf},
isbn = {1-55860-800-1},
journal = {Statistics},
mendeley-groups = {PPL},
number = {2},
pages = {362--369},
title = {{Expectation Propagation for Approximate Bayesian Inference F d}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1319{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {17},
year = {1999}
}

@article{Zhang1994,
abstract = {The general problem of computing posterior probabilities in Bayesian networds is NP-hard (Cooper 1990). However efficient algorithms are often possible for particular applications by exploiting problem structures. It is well understood that the key to the materialization of such a possibility is to make use of conditional independence and work with factorizations of joint probabilities rather than joint probabilities themselves. Differnent exact approaches can be characterized in terms of their choices of factorizations. We propose a new approach which adopts a straightforward way for factorizing joint probabilities. In comparison with the clique tree propagation approach, our approach is very simple. It allows the pruning of irrelevant variables, it accommodates changes to the knowledge base more easily. It is easier to implement. More importantly, it can be adapted to utilize both intercausal independence and conditional independence in one uniform framework. On the other hand, clique tree propagation is better in terms of facilitating pre-computations.},
author = {Zhang, Nevin L and Poole, D},
file = {:home/gcandal/Downloads/canai94.pdf:pdf},
issn = {0710-0825},
journal = {Proc. of the Tenth Canadian Conference on Artificial Intelligence},
keywords = {probabilistic graphical models},
mendeley-groups = {ML/Prob/Bayes},
pages = {171--178},
title = {{A simple approach to Bayesian network computations}},
url = {http://hdl.handle.net/1783.1/757},
year = {1994}
}

@article{Winn2005,
abstract = {This paper presents Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to a Bayesian Network. Like belief propagation, Variational Message Passing proceeds by passing messages between nodes in the graph and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing ad- ditional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Mes- sage Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be specified graphically and then solved variationally without recourse to coding.},
author = {Winn, John and Bishop, Cm and Jaakkola, T},
doi = {10.1007/s002130100880},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winn, Bishop, Jaakkola - 2005 - Variational Message Passing.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,variational methods},
mendeley-groups = {PPL},
pages = {661--694},
pmid = {11713616},
title = {{Variational Message Passing}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=15324435{\&}AN=18003399{\&}h=hVptawIRxY9bpe0GokNI9+sC/7JSK74XN+ztPcGsGNYZQY5RlIgU6Iy2WzbduRcHJXoqZMRPgfHZO18TF6s+Kw=={\&}crl=c},
volume = {6},
year = {2005}
}

@article{Minka1999,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, Expectation Propagation, unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propaga- tion in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expec- tation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models showExpectation Propagation to be convincingly better than methods with similar computational cost: Laplaces method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P},
eprint = {1301.2294},
file = {:home/gcandal/Downloads/minka-ep-uai.pdf:pdf},
isbn = {1-55860-800-1},
journal = {Statistics},
mendeley-groups = {PPL},
number = {2},
pages = {362--369},
title = {{Expectation Propagation for Approximate Bayesian Inference F d}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1319{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {17},
year = {1999}
}

@article{Shen2010,
abstract = {n recent work we have developed a novel variational inference method for partially observed systems governed by stochastic differential equations. In this paper we provide a comparison of the Variational Gaussian Process Smoother with an exact solution computed using a Hybrid Monte Carlo approach to path sampling, applied to a stochastic double well potential model. It is demonstrated that the variational smoother provides us a very accurate estimate of mean path while conditional variance is slightly underestimated. We conclude with some remarks as t o the advantages and disadvantages of the variational smoother.},
author = {Shen, Yuan and Archambeau, Cedric and Cornford, Dan and Opper, Manfred and Shawe-taylor, John and Barillec, Remi},
doi = {10.1007/s11265-008-0299-y},
file = {:home/gcandal/Downloads/jsps{\_}ys09{\_}web.pdf:pdf},
issn = {1939-8018},
journal = {Journal of Signal Processing Systems},
mendeley-groups = {PPL},
number = {1},
pages = {51--59},
title = {{A comparison of variational and Markov Chain Monte Carlo methods for inference in partially observed stochastic dynamic systems}},
volume = {61},
year = {2010}
}

@article{Myers1986,
abstract = {There has been a great interest recently in systems that use graphics to aid in the programming, debugging, and understanding of computer programs. The terms “Visual Programming” and “Program Visualization” have been applied to these systems. Also, there has been a renewed interest in using examples to help alleviate the complexity of programming. This technique is called “Programming by Example.” This paper attempts to provide more meaning to these terms by giving precise definitions, and then uses these definitions to classify existing systems into a taxonomy. A number of common unsolved problems with most of these systems are also listed.},
author = {Myers, B. a.},
doi = {10.1145/22339.22349},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Myers - 1986 - Visual programming, programming by example, and program visualization a taxonomy.pdf:pdf},
isbn = {0897911806},
issn = {07366906},
journal = {ACM SIGCHI Bulletin},
keywords = {and phrases,ming,program visualization,programming by example,visual rogram-},
mendeley-groups = {VPL},
number = {4},
pages = {59--66},
title = {{Visual programming, programming by example, and program visualization: a taxonomy}},
volume = {17},
year = {1986}
}

@article{Burnett1999,
author = {Burnett, Margaret},
doi = {10.1002/047134608X.W1707},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burnett - 1999 - Visual programming.pdf:pdf},
issn = {01676423},
journal = {Wiley Encyclopedia of Electrical and Electronics Engineering},
mendeley-groups = {VPL},
number = {1-3},
pages = {275--283},
title = {{Visual programming}},
url = {ftp://ftp.cs.orst.edu/pub/burnett/whatIsVP.pdf},
volume = {32},
year = {1999}
}

@article{JamalRahmanandWenzel2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Jamal, Rahman and Wenzel}, Lothar},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jamal, Rahman and Wenzel - 2014 - The Applicability of Visual Programming to Large Real-World Applications.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Igarss 2014},
keywords = {Bott},
mendeley-groups = {VPL},
number = {1},
pages = {1--5},
pmid = {15003161},
title = {{The Applicability of Visual Programming to Large Real-World Applications}},
year = {2014}
}

@article{Shu1988,
author = {Green, T.R.G.},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shu - 1988 - Visual Programming.pdf:pdf},
journal = {The British Computer Society • Human-Computer Interaction Group},
mendeley-groups = {VPL},
title = {{Noddy's Guide to ... Visual Programming}},
year = {1995}
}

@article{Myers1990,
abstract = {There has been a great interest recently in systems that use graphics to aid in the programming, debugging, and understanding of computer systems. The terms "Visual Programming" and "Program Visualization" have been applied to these systems. This paper attempts to provide more meaning to these terms by giving precise definitions, and then surveys a number of systems that can be classified as providing Visual Programming or Program Visualization. These systems are organized by classifying them into three different taxonomies.},
author = {Myers, Brad A.},
doi = {10.1016/S1045-926X(05)80036-9},
file = {:home/gcandal/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Myers - 1990 - Taxonomies of visual programming and program visualization.pdf:pdf},
issn = {1045926X},
journal = {Journal of Visual Languages {\&} Computing},
mendeley-groups = {VPL},
number = {1},
pages = {97--123},
title = {{Taxonomies of visual programming and program visualization}},
volume = {1},
year = {1990}
}

@article{Lewis1987,
author = {Lewis, G and Olson, G},
file = {:home/gcandal/Downloads/awarchive.pdf:pdf},
isbn = {0-89391-461-4},
journal = {Empirical Studies of Programmers: Second Workshop},
mendeley-groups = {VPL},
number = {15},
pages = {248--263},
title = {{Can principles of cognition lower rhw barriers to programming?}},
volume = {2},
year = {1987}
}

@phdthesis{Halbert1984,
author = {Halbert, Daniel Conrad},
mendeley-groups = {VPL},
school = {University of California, Berkeley},
title = {{Programming by Example}},
type = {PhD},
url = {http://www.danhalbert.org/pbe-html.htm},
year = {1984}
}

@Inbook{Clarisse1986,
author="Clarisse, Olivier
and Chang, Shi-Kuo",
editor="Chang, Shi-Kuo
and Ichikawa, Tadao
and Ligomenides, Panos A.",
chapter="Vicon",
title="Visual Languages",
year="1986",
publisher="Springer US",
address="Boston, MA",
pages="151--190",
isbn="978-1-4613-1805-7",
doi="10.1007/978-1-4613-1805-7_7",
url="http://dx.doi.org/10.1007/978-1-4613-1805-7_7"
}

@article{cunniff1986does,
  title={Does programming language affect the type of conceptual bugs in beginners' programs? A comparison of FPL and Pascal},
  author={Cunniff, Nancy and Taylor, Robert P and Black, John B},
  journal={ACM SIGCHI Bulletin},
  volume={17},
  number={4},
  pages={175--182},
  year={1986},
  publisher={ACM}
}

@inproceedings{ambler1987forms,
  title={Forms: Expanding the visualness of sheet languages},
  author={Ambler, Allen},
  booktitle={1987 Workshop on Visual Languages},
  pages={105--117},
  year={1987}
}

@article{Brooks1986,
abstract = {All software construction involves essential tasks, the fashioning of the complex conceptual structures that compose the abstract software entity, and accidental tasks, the representation of these abstract entities in programming languages and the mapping of these onto machine languages within space and speed constraints. Most of the big past gains in software productivity have come from removing artificial barriers that have made the accidental tasks inordinately hard, such as severe hardware constraints, awkward programming languages, lack of machine time. How much of what software engineers now do is still devoted to the accidental, as opposed to the essential? Unless it is more than 9/10 of all effort, shrinking all the accidental activities to zero time will not give an order of magnitude improvement. Therefore it appears that the time has come to address the essential parts of the software task, those concerned with fashioning abstract conceptual structures of great complexity. I suggest: • Exploiting the mass market to avoid constructing what can be bought. • Using rapid prototyping as part of a planned iteration in establishing software requirements. • Growing software organically, adding more and more function to systems as they are run, used, and tested. • Identifying and developing the great conceptual designers of the rising generation. Introduction},
author = {Brooks, Frederick Phillips Jr.},
doi = {10.1109/MC.1987.1663532},
file = {:home/gcandal/Downloads/Brooks-NoSilverBullet.pdf:pdf},
isbn = {0444700773},
issn = {00189162},
journal = {Proceedings of the IFIP Tenth World Computing Conference},
mendeley-groups = {VPL},
pages = {1069--1076},
pmid = {8804735756196452418},
title = {{No silver bullet-essence and accidents of software engineering}},
url = {http://www.cgl.ucsf.edu/Outreach/pc204/NoSilverBullet},
year = {1986}
}

@article{dijkstra1989cruelty,
  title={On the cruelty of really teaching computing science},
  author={Dijkstra, Edsger W and others},
  journal={Communications of the ACM},
  volume={32},
  number={12},
  pages={1398--1404},
  year={1989}
}
